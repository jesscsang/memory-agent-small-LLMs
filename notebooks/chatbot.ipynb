{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Vous aimez programmmer.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:06.4255961Z', 'done': True, 'done_reason': 'stop', 'total_duration': 871054300, 'load_duration': 16841300, 'prompt_eval_count': 42, 'prompt_eval_duration': 595000000, 'eval_count': 7, 'eval_duration': 257000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-880a5e6d-bbeb-4240-80a5-2d2e9a23909a-0', usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2:1b-instruct-fp16\"\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = local_llm,\n",
    "    # temperature = 0.8,\n",
    "    # num_predict = 256,\n",
    "    # format=\"json\",\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "# Test message\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless Nature of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! How's it going?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:06.9651073Z', 'done': True, 'done_reason': 'stop', 'total_duration': 529604100, 'load_duration': 17111200, 'prompt_eval_count': 30, 'prompt_eval_duration': 175000000, 'eval_count': 9, 'eval_duration': 337000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1c3db5d8-fbf9-4e31-ab14-959390809223-0', usage_metadata={'input_tokens': 30, 'output_tokens': 9, 'total_tokens': 39})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about you, so I'm not aware of your name. This is the beginning of our conversation, and I'm here to help with any questions or topics you'd like to discuss. Is there something specific you'd like to talk about or ask about?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:09.6692977Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2694440400, 'load_duration': 12655400, 'prompt_eval_count': 30, 'prompt_eval_duration': 166000000, 'eval_count': 58, 'eval_duration': 2515000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-9d2f7c27-6a8f-471a-ac49-93c236170241-0', usage_metadata={'input_tokens': 30, 'output_tokens': 58, 'total_tokens': 88})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about you, Bob. I'm a large language model, I don't have the ability to store or recall personal data about individual users. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. Is there something else I can help you with?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:13.1318953Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3452264300, 'load_duration': 12995300, 'prompt_eval_count': 55, 'prompt_eval_duration': 563000000, 'eval_count': 68, 'eval_duration': 2874000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-6b6fe42a-2241-4b86-9e00-c09660281a8c-0', usage_metadata={'input_tokens': 55, 'output_tokens': 68, 'total_tokens': 123})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! How's it going?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I made a mistake by not addressing you by your name earlier. You asked me to call you \"Bob\", and I should have done that. Let's start fresh! What's on your mind, Bob?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hi! I'm Bob.\", additional_kwargs={}, response_metadata={}, id='ca4bc3dd-e723-421a-8722-6d2ea511a256'),\n",
       " AIMessage(content=\"Hello Bob! How's it going?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:13.6923631Z', 'done': True, 'done_reason': 'stop', 'total_duration': 521435100, 'load_duration': 13125200, 'prompt_eval_count': 31, 'prompt_eval_duration': 165000000, 'eval_count': 9, 'eval_duration': 342000000, 'message': {'role': 'assistant', 'content': '', 'images': None, 'tool_calls': None}}, id='run-9ba91e06-2a53-4291-90fc-8efe8d51e3bc-0', usage_metadata={'input_tokens': 31, 'output_tokens': 9, 'total_tokens': 40}),\n",
       " HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='0f73a052-a89b-44bd-a175-d79f68bb48e4'),\n",
       " AIMessage(content='I made a mistake by not addressing you by your name earlier. You asked me to call you \"Bob\", and I should have done that. Let\\'s start fresh! What\\'s on your mind, Bob?', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2025-04-15T03:47:15.7093025Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2004605200, 'load_duration': 12918200, 'prompt_eval_count': 54, 'prompt_eval_duration': 171000000, 'eval_count': 43, 'eval_duration': 1818000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-77ee35c5-e027-4abc-94d9-0ce59bc0739a-0', usage_metadata={'input_tokens': 54, 'output_tokens': 43, 'total_tokens': 97})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
